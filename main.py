# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i0EteR4ge-TM99r5ilZ6q5-NCFjth1aR
"""

from fastapi import FastAPI, Request
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import torch
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template

app = FastAPI()

# Optional: allow CORS for testing
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load model and tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

FastLanguageModel.for_inference(model)
tokenizer = get_chat_template(tokenizer, chat_template="chatml")

class Question(BaseModel):
    question: str

def moderate_input(text):
    banned_keywords = ["kill", "suicide", "hack", "illegal", "scam", "jailbreak"]
    return any(kw in text.lower() for kw in banned_keywords)

@app.post("/ask")
async def ask_question(data: Question):
    user_input = data.question.strip()

    # Basic moderation
    if moderate_input(user_input):
        return {"answer": "I'm sorry, I can't help with that request."}

    # Format chat prompt
    messages = [
        {"from": "human", "value": f"Answer the following finance question simply: {user_input}"},
        {"from": "gpt", "value": ""},
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    model.to(inputs.input_ids.device)

    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Post-process output
    answer = decoded.split("\n")[-1].strip()
    return {"answer": answer}

