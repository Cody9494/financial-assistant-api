# -*- coding: utf-8 -*-
"""BANKOFCYPRUS_EXERCISEipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QEiw0j1_1QKfsxEfXG6U0-sAtevJj2B
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastLanguageModel
import torch

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",
    "unsloth/gemma-2b-bnb-4bit",
] # More models at https://huggingface.co/unsloth

max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype          = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit   = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype          = dtype,
    load_in_4bit   = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r            = 16,     # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    lora_alpha   = 16,
    lora_dropout = 0,      # Supports any, but = 0 is optimized
    bias         = "none", # Supports any, but = "none" is optimized

    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],

    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state               = 3407,
    use_rslora                 = False,  # We support rank stabilized LoRA
    loftq_config               = None,   # And LoftQ
)

FastLanguageModel.for_inference(model)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "chatml", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth
    mapping       = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)

# Define structured conversation
messages = [
    {"from": "system", "value": "You are a helpful and trustworthy financial assistant."},
    {"from": "user", "value": "How can I start budgeting?"}
]

print(tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True))

from transformers import TextStreamer

inputs = tokenizer([tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)], return_tensors = "pt").to("cuda")

text_streamer = TextStreamer(tokenizer)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,

    # repetition_penalty   = 1.0,
    # no_repeat_ngram_size = 2,
    # top_k                = 50,
    # top_p                = 0.9
)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,

    repetition_penalty   = 1.0,
    no_repeat_ngram_size = 2,
    top_k                = 50,
    top_p                = 0.9
)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# from unsloth import FastLanguageModel
# from unsloth.chat_templates import get_chat_template
# import torch
# 
# app = FastAPI()
# 
# class QuestionRequest(BaseModel):
#     question: str
# 
# model_name = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"
# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name=model_name,
#     max_seq_length=2048,
#     dtype=None,
#     load_in_4bit=True,
# )
# FastLanguageModel.for_inference(model)
# 
# tokenizer = get_chat_template(
#     tokenizer,
#     chat_template="chatml",
#     mapping={"role": "from", "content": "value", "user": "human", "assistant": "gpt"},
#     map_eos_token=True,
# )
# 
# @app.post("/ask")
# async def ask_question(request: QuestionRequest):
#     question = request.question.strip()
# 
#     messages = [
#         {"from": "system", "value": "You are a helpful and trustworthy financial assistant."},
#         {"from": "user", "value": question}
#     ]
# 
#     prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
#     inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
# 
#     with torch.no_grad():
#         outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7)
#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()
# 
#     return {"answer": answer}
#

from google.colab import files
files.download('main.py')

import subprocess
subprocess.run(['pip', 'install', 'gradio', 'transformers'])

import gradio as gr
from transformers import pipeline

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… (CPU-friendly)
generator = pipeline("text-generation", model="distilgpt2")

# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï€Î¿Ï… ÎºÎ±Î»ÎµÎ¯Ï„Î±Î¹ ÏŒÏ„Î±Î½ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï€Î±Ï„Î®ÏƒÎµÎ¹ submit
def respond_to_prompt(prompt):
    result = generator(prompt, max_length=100, do_sample=True)[0]["generated_text"]
    return result.strip()

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± GUI
interface = gr.Interface(
    fn=respond_to_prompt,
    inputs=gr.Textbox(label="Ask me something"),
    outputs=gr.Textbox(label="Answer"),
    title="ğŸ§  GenAI Financial Assistant",
    description="Type a question and get an AI-generated answer."
)

# Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· ÏƒÎµ browser
interface.launch(share=True)  # share=True Î³Î¹Î± public URL ÏƒÏ„Î¿ Colab

import gradio as gr
import torch

def generate_response(prompt):
    # Format the prompt (can adapt based on your tokenizer setup)
    messages = [
        {"from": "system", "value": "You are a helpful assistant."},
        {"from": "user", "value": prompt}
    ]

    # Chat-style prompt (if using chat_template like chatml)
    formatted_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            streamer=text_streamer,
            max_new_tokens=512,
            temperature=0.1,
            # Optional tuning:
            # top_k=50,
            # top_p=0.9,
            # no_repeat_ngram_size=2
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True).replace(formatted_prompt, "").strip()
    return response

# Build Gradio UI
gr.Interface(
    fn=generate_response,
    inputs=gr.Textbox(label="Enter your question"),
    outputs=gr.Textbox(label="Answer"),
    title="GenAI Financial Assistant",
    description="Type a question and get an AI-generated answer.",
).launch(share=True)

