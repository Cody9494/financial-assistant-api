# -*- coding: utf-8 -*-
"""BANKOFCYPRUS_EXERCISEipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QEiw0j1_1QKfsxEfXG6U0-sAtevJj2B
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastLanguageModel
import torch

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",
    "unsloth/gemma-2b-bnb-4bit",
] # More models at https://huggingface.co/unsloth

max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype          = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit   = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype          = dtype,
    load_in_4bit   = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r            = 16,     # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    lora_alpha   = 16,
    lora_dropout = 0,      # Supports any, but = 0 is optimized
    bias         = "none", # Supports any, but = "none" is optimized

    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],

    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state               = 3407,
    use_rslora                 = False,  # We support rank stabilized LoRA
    loftq_config               = None,   # And LoftQ
)

FastLanguageModel.for_inference(model)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "chatml", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth
    mapping       = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)

# Define structured conversation
messages = [
    {"from": "system", "value": "You are a helpful and trustworthy financial assistant."},
    {"from": "user", "value": "How can I start budgeting?"}
]

print(tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True))

from transformers import TextStreamer

inputs = tokenizer([tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)], return_tensors = "pt").to("cuda")

text_streamer = TextStreamer(tokenizer)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,

    # repetition_penalty   = 1.0,
    # no_repeat_ngram_size = 2,
    # top_k                = 50,
    # top_p                = 0.9
)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,

    repetition_penalty   = 1.0,
    no_repeat_ngram_size = 2,
    top_k                = 50,
    top_p                = 0.9
)

# Define structured conversation
messages = [
    {"from": "system", "value": "You are a helpful and trustworthy financial assistant."},
    {"from": "user", "value": "How can I start investing in the stock market?"}
]

inputs = tokenizer([tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)], return_tensors = "pt").to("cuda")

text_streamer = TextStreamer(tokenizer)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,
)

# Define structured conversation
messages = [
    {"from": "system", "value": "You are a helpful and trustworthy financial assistant."},
    {"from": "user", "value": "How can i protect my financial info from scams?"}
]

inputs = tokenizer([tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)], return_tensors = "pt").to("cuda")

text_streamer = TextStreamer(tokenizer)

_ = model.generate(
    **inputs,
    streamer             = text_streamer,
    max_new_tokens       = 512,
    temperature          = 0.1,
)

